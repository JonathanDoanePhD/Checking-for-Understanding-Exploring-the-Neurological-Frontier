{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fad4d75-6176-491b-bc54-63746268f082",
   "metadata": {},
   "source": [
    "# 2. Model 1 (BiLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b3ba2-8632-4ebc-8553-155fca66751b",
   "metadata": {},
   "source": [
    "SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40856a17-03e7-4bc5-ae4f-e28d0e5a0820",
   "metadata": {},
   "source": [
    "<div id='section2a'style='text-align: right'><a href='#toc'><hr>table_of_contents</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebfce4-1191-459c-b016-f3fd3c2cb524",
   "metadata": {},
   "source": [
    "## 2.A. BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2903a1b-2e4c-41d7-8e58-13ecd584f46f",
   "metadata": {},
   "source": [
    "### Defining the Model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "837f98f2-7d35-4b71-8b01-929e092369c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 17, 1)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 17, 64)            128       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 17, 512)          657408    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 512)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 17, 256)          656384    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 17, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4352)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               557184    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,871,233\n",
      "Trainable params: 1,871,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = K.Input(shape=(17, 1))\n",
    "\n",
    "dense_1 = Dense(64, activation='relu', kernel_regularizer=K.regularizers.l2())(inputs)\n",
    "lstm_1 =  Bidirectional(LSTM(256, return_sequences=True))(dense_1)\n",
    "drop_1 = Dropout(0.3)(lstm_1)\n",
    "\n",
    "lstm_2 =  Bidirectional(LSTM(128, return_sequences=True))(drop_1)\n",
    "drop_2 = Dropout(0.3)(lstm_2)\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "\n",
    "dense_2 = Dense(128, activation='relu')(flat)\n",
    "outputs = Dense(1, activation='sigmoid')(dense_2)\n",
    "\n",
    "model = K.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb4d71-fbc5-47c3-8ae1-f1f032737d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization:\n",
    "    \n",
    "    K.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a257c-1ed6-4d63-a1e1-7282f912cb5c",
   "metadata": {},
   "source": [
    "We will be training for 100 epochs starting with learning_rate = 0.001 and batch_size = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2078d44-3e25-4a8e-a81c-1d1f6cf422eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, save_to, epoch=epoch_num):\n",
    "\n",
    "        opt_adam = K.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "        lr_schedule = K.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "        \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                  loss=['binary_crossentropy'],\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train,\n",
    "                        batch_size=20,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=[es, mc, lr_schedule])\n",
    "        \n",
    "        saved_model = load_model(save_to + '_best_model.h5')\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "def train_modell(model, X_train, y_train, X_test, y_test, save_to, kfold_order=None, epoch=epoch_num):\n",
    "        if not kfold_order:\n",
    "            kfold_order = 'singleTrial'\n",
    "        else:\n",
    "            print(\"=============================================================\")\n",
    "            print(\"Training for fold no \"+str(kfold_order))\n",
    "            print(\"=============================================================\")\n",
    "        opt_adam = K.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_'+str(kfold_order)+'.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "        lr_schedule = K.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "        \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                  loss=['binary_crossentropy'],\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train,\n",
    "                        batch_size=20,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=[es, mc, lr_schedule], verbose=0)\n",
    "        \n",
    "        saved_model = load_model(save_to + '_best_model_'+str(kfold_order)+'.h5')\n",
    "\n",
    "        # from matplotlib.pylab import rcParams\n",
    "        rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "318e536c-5023-4192-b1c8-fc6217b38eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(model, X_train, y_train, X_test, y_test, save_to='./', epoch=epoch_num)#100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41cb5a-77b8-4559-94ac-3bbff1b52a4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.B. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2216658-e599-46ee-885b-ac7cd9d851c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gridsearch:\n",
    "    \n",
    "    \n",
    "    batch_size = [16, 32]\n",
    "    epochs = [epoch_num, epoch_num] #10, 20\n",
    "    optimizers = ['adam', 'rmsprop']\n",
    "    param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizers)\n",
    "    param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea62fd5-b617-466d-939e-efb847d2ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNS FOREVER... 16adam, 16r, 32adam, 32r -> each could be 2.0 or 2.00 (so eight). In either case that' s2 epochs, so cv=5 times 16, total 80\n",
    "if gridsearch:\n",
    "    \n",
    "    \n",
    "    def LSTM_Model(optimizer='adam'):\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(17, 1)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    model = KerasClassifier(build_fn=LSTM_Model, verbose=1)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=folds_num) # 5\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b1c16-5b00-4113-a330-3f5048611b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training\n",
    "batch_size = 64\n",
    "\n",
    "lstm_model = LSTM_Model()\n",
    "\n",
    "lstm_hist = lstm_model.fit(X_train, y_train,\n",
    "                           validation_data=(X_test, y_test),\n",
    "                           epochs=epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b97a72-0cc8-47fe-be29-8e99b5f08072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization:\n",
    "    \n",
    "    \n",
    "    def classify(X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                      random_state=117, stratify=y)\n",
    "\n",
    "        model = xgboost.XGBClassifier(base_score=0.5, learning_rate=0.1, max_depth=6,\n",
    "              objective='binary:logistic', eta=0.01)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "        ## draw the tree\n",
    "\n",
    "        rcParams['figure.figsize'] = 80, 50\n",
    "\n",
    "        plot_tree(model)\n",
    "        plt.show()\n",
    "\n",
    "        ## show the cross validation result\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=folds_num)\n",
    "        results = cross_val_score(model, X, y, cv=kfold)\n",
    "        print(\"Cross Validation Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "        print(\"\")\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "\n",
    "        ## print feature importance\n",
    "        print(\"Feature Importance\")\n",
    "        rcParams['figure.figsize'] = 5, 5\n",
    "        plot_importance(model)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e99aa-0f16-4e3a-9d34-78d5888f1dcd",
   "metadata": {},
   "source": [
    "### Classification using Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fac116-bf59-4fdd-b4e9-9182e0247388",
   "metadata": {},
   "source": [
    "This method is based on the work of Shreyas P.J., and you can refer to the original source at https://www.kaggle.com/code/shreyaspj/confused-student-eeg-prediction/notebook. However, in this study, we have applied moving averages to the data instead of using the original data, and have focused solely on frequency-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4158a9-0c6a-42c4-a136-3345760ba84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ma = df_ma[waves]\n",
    "\n",
    "X_lstm = StandardScaler().fit_transform(X_ma)\n",
    "y_lstm = y_ma\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42, stratify=y_lstm)\n",
    "\n",
    "\n",
    "n_features = X_ma.shape[1]\n",
    "X_train = np.array(X_train).reshape(-1, n_features, 1)\n",
    "X_test = np.array(X_test).reshape(-1, n_features, 1)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17611b35-76aa-4c38-abf4-bd4ca056caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS IS A REPEAT\n",
    "\n",
    "\n",
    "# inputs = K.Input(shape=(n_features, 1))\n",
    "\n",
    "# dense_1 = Dense(64, activation='relu', kernel_regularizer=K.regularizers.l2())(inputs)\n",
    "# lstm_1 =  Bidirectional(LSTM(256, return_sequences=True))(dense_1)\n",
    "# drop_1 = Dropout(0.3)(lstm_1)\n",
    "# lstm_2 =  Bidirectional(LSTM(128, return_sequences=True))(drop_1)\n",
    "# drop_2 = Dropout(0.3)(lstm_2)\n",
    "# flat = Flatten()(drop_2)\n",
    "# dense_2 = Dense(128, activation='relu')(flat)\n",
    "# outputs = Dense(1, activation='sigmoid')(dense_2)\n",
    "\n",
    "# model = K.Model(inputs, outputs)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775e0e7-017c-4694-8655-9fac0994add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualization:\n",
    "    \n",
    "    \n",
    "    K.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6425f-0d85-4fe9-8187-c23ee491e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, save_to, kfold_order=None, epoch=epoch_num):\n",
    "        if not kfold_order:\n",
    "            kfold_order = 'singleTrial'\n",
    "        else:\n",
    "            print(\"=============================================================\")\n",
    "            print(\"Training for fold no \"+str(kfold_order))\n",
    "            print(\"=============================================================\")\n",
    "        opt_adam = K.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_'+str(kfold_order)+'.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "        lr_schedule = K.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "        \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                  loss=['binary_crossentropy'],\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train,\n",
    "                        batch_size=20,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=[es, mc, lr_schedule], verbose=0)\n",
    "        \n",
    "        saved_model = load_model(save_to + '_best_model_'+str(kfold_order)+'.h5')\n",
    "        \n",
    "        if visualization:\n",
    "            \n",
    "            \n",
    "            # from matplotlib.pylab import rcParams\n",
    "            \n",
    "            rcParams['figure.figsize'] = 5, 5\n",
    "            \n",
    "            plt.plot(history.history['accuracy'])\n",
    "            plt.plot(history.history['val_accuracy'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            # summarize history for loss\n",
    "            \n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37c125-2604-4b39-9a0d-013a94793b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(model, X_train, y_train, X_test, y_test, save_to='./', epoch=epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f8167-1685-4ad7-be1a-bdf7e604fb66",
   "metadata": {},
   "source": [
    "Our results demonstrate that applying a moving average and classifying using bidirectional LSTM has shown promising results, achieving accuracy of 92% with validation accuracy of 89%.\n",
    "\n",
    "Next, we will validate the results once again using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56148f-760c-42b8-92ba-b2971b460a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "fold_var = 1\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "skf = StratifiedKFold(n_splits=folds_num, random_state=117, shuffle=True) \n",
    "\n",
    "for train_index, val_index in skf.split(X_lstm, y_lstm):\n",
    "    X_train = X_lstm[train_index]\n",
    "    y_train = y_lstm.iloc[train_index]\n",
    "    X_test = X_lstm[val_index]\n",
    "    y_test = y_lstm.iloc[val_index]\n",
    "\n",
    "\n",
    "    model, history = train_model(model, X_train, y_train, X_test, y_test, save_to='./', kfold_order=fold_var, epoch=epoch_num) #100\n",
    "\n",
    "    # LOAD BEST MODEL to evaluate the performance of the model\n",
    "    model.load_weights(\"./_best_model_\"+str(fold_var)+\".h5\")\n",
    "\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    K.backend.clear_session()\n",
    "\n",
    "    fold_var += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
